{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt # creating visualizations\n",
    "import numpy as np # basic math and random numbers\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch # package for building functions with learnable parameters\n",
    "import torch.nn as nn # prebuilt functions specific to neural networks\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable # storing data while learning\n",
    "from torch.distributions import constraints, multivariate_normal, Normal\n",
    "from torch.distributions.distribution import Distribution\n",
    "from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First loading the recorded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoordinateDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, file_name, with_scaler=False):\n",
    "        file_out = pd.read_csv(file_name, names=['ball_x', 'ball_y', 'positionLeftShoulderRoll', 'positionLeftShoulderPitch'])\n",
    "        print(f\"Number of samples: {len(file_out)}\")\n",
    "        file_out.drop_duplicates(subset=['ball_x','ball_y'], inplace=True)\n",
    "        print(f\"Number of samples after removing duplicates: {len(file_out)}\")\n",
    "        x = file_out[['ball_x','ball_y']].values\n",
    "        y = file_out[['positionLeftShoulderRoll', 'positionLeftShoulderPitch']].values\n",
    "        x_train = x\n",
    "        if with_scaler:\n",
    "            sc = StandardScaler()\n",
    "            x_train = sc.fit_transform(x_train)\n",
    "        y_train = y\n",
    "        x_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "        self.x_train = Variable(x_tensor)\n",
    "        self.y_train = Variable(y_tensor, requires_grad=False)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "        \n",
    "dataset = CoordinateDataset('../controllers/tutorial2_tracker/data_points.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then training the MDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output, n_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        \n",
    "        #IMPORTANT notes\n",
    "        # - Use softmax activation for pi  (they need to add up to 1)\n",
    "        # - Use exponential linear unit for deviations (they should not be negative or close to zero)\n",
    "        self.pis = nn.Linear(n_hidden, n_gaussians) # Coefficents\n",
    "        self.mus = nn.Linear(n_hidden, n_gaussians * n_output) # Means\n",
    "        self.sigmas = nn.Linear(n_hidden, n_gaussians) # Variances / Deviations\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #Program the forward pass\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        pi = F.softmax(self.pis(x), -1)\n",
    "        sigma = torch.exp(self.sigmas(x))\n",
    "        mu = self.mus(x)\n",
    "        \n",
    "        return pi, sigma, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneDivSqrtTwoPI = 1.0 / np.sqrt(2.0*np.pi)\n",
    "def gaussian_distribution(y, mu, sigma):\n",
    "    result = (y - mu) * torch.reciprocal(sigma)\n",
    "    result = -0.5 * (result * result)\n",
    "    return (torch.exp(result) * torch.reciprocal(sigma)) * oneDivSqrtTwoPI\n",
    "\n",
    "def loss_fn(pi, sigma, mu, y):\n",
    "    N, K = pi.shape\n",
    "    _, KT = mu.shape\n",
    "    NO = int(KT / K)\n",
    "    losses = Variable(torch.zeros(N, K))\n",
    "    for i in range(K): \n",
    "        window_mu = mu[:, i*NO:(i+1)*NO]\n",
    "        sigma_expanded = sigma[:, i].unsqueeze(-1).repeat(1, NO)\n",
    "        likelihood_z_x = -torch.log(gaussian_distribution(y, window_mu, sigma_expanded))\n",
    "        prior_z = pi[:, i]\n",
    "        losses += torch.mean(prior_z.unsqueeze(-1).repeat(1, NO) * likelihood_z_x)\n",
    "    loss = torch.mean(losses)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-662c1317883f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMDN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "from .. import MDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGAUSSIANS = 5\n",
    "EPOCHS = 10000\n",
    "model = MDN(n_input=2, n_hidden=20, n_output=2, n_gaussians=NGAUSSIANS)\n",
    "optimizer = torch.optim.Adam(model.parameters()) # Select optimizer\n",
    "losses = []\n",
    "\n",
    "def train_mdn(network, x_train, y_train):\n",
    "    for epoch in range(EPOCHS):\n",
    "        pi_variable, sigma_variable, mu_variable = network(x_train)\n",
    "        loss = loss_fn(pi_variable, sigma_variable, mu_variable, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.154797077178955\n",
      "500 -1.2244654893875122\n",
      "1000 -1.5054270029067993\n",
      "1500 -1.9656215906143188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-86623b6bec7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_mdn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-6fb9d7cd34b8>\u001b[0m in \u001b[0;36mtrain_mdn\u001b[1;34m(network, x_train, y_train)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mpi_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_variable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpi_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu_variable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-44-72099d4ebad4>\u001b[0m in \u001b[0;36mloss_fn\u001b[1;34m(pi, sigma, mu, y)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mwindow_mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mNO\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mNO\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0msigma_expanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNO\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mlikelihood_z_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma_expanded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mprior_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprior_z\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNO\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlikelihood_z_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_mdn(model, dataset.x_train, dataset.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'testmdn_model_nao_{int(EPOCHS/1000)}kepochs_{NGAUSSIANS}gaussians')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(FFN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_input, n_hidden)\n",
    "        self.fc2 = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ffn = FFN(n_input=2, n_hidden=6, n_output=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ffn(network, x_train, y_train, optimizer, criterion, n_epoch=10000):\n",
    "    for epoch in range(n_epoch):\n",
    "        prediction = network(x_train)\n",
    "        loss = criterion(prediction, y_train)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if epoch % 500 == 0:\n",
    "            print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1683187037706375\n",
      "500 0.014715773053467274\n",
      "1000 0.014120973646640778\n",
      "1500 0.016923552379012108\n",
      "2000 0.013578483834862709\n",
      "2500 0.013523253612220287\n",
      "3000 0.013592777773737907\n",
      "3500 0.013481817208230495\n",
      "4000 0.008557140827178955\n",
      "4500 0.006724161561578512\n",
      "5000 0.007200705818831921\n",
      "5500 0.007027440704405308\n",
      "6000 0.0066414386965334415\n",
      "6500 0.007104900199919939\n",
      "7000 0.010111912153661251\n",
      "7500 0.006551699712872505\n",
      "8000 0.006863217800855637\n",
      "8500 0.00658069271594286\n",
      "9000 0.00652605714276433\n",
      "9500 0.008215435780584812\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model_ffn.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss() \n",
    "\n",
    "train_ffn(model_ffn, dataset.x_train, dataset.y_train, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ffn.state_dict(), 'ffn_model_nao')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
